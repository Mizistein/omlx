# SPDX-License-Identifier: Apache-2.0
"""
Model discovery for oMLX multi-model serving.

This module scans a model directory and discovers available models,
estimating memory usage for each.

Supports:
- LLM models: Use BatchedEngine for continuous batching with paged KV cache
- Embedding models: Use EmbeddingEngine for batch embedding generation
- Reranker models: Use RerankerEngine for document reranking
"""

import json
import logging
from dataclasses import dataclass
from pathlib import Path
from typing import Literal

logger = logging.getLogger(__name__)

ModelType = Literal["llm", "embedding", "reranker"]
EngineType = Literal["batched", "embedding", "reranker"]

# Known embedding model types from mlx-embeddings
EMBEDDING_MODEL_TYPES = {
    "bert",
    "xlm-roberta",
    "xlm_roberta",
    "modernbert",
    "qwen3",
    "gemma3-text",
    "gemma3_text",
    "siglip",
    "colqwen2_5",
    "colqwen2-5",
    "lfm2",
}

# Known embedding architectures
EMBEDDING_ARCHITECTURES = {
    "BertModel",
    "BertForMaskedLM",
    "XLMRobertaModel",
    "XLMRobertaForMaskedLM",
    "ModernBertModel",
    "ModernBertForMaskedLM",
    "Qwen3ForTextEmbedding",
    "SiglipModel",
    "SiglipVisionModel",
    "SiglipTextModel",
}

# Supported reranker architectures
SUPPORTED_RERANKER_ARCHITECTURES = {
    "ModernBertForSequenceClassification",  # via mlx-embeddings
    "XLMRobertaForSequenceClassification",  # omlx native implementation
}

# Unsupported reranker architectures (future support)
UNSUPPORTED_RERANKER_ARCHITECTURES = {
    "BertForSequenceClassification",
    "Qwen3ForSequenceClassification",
}

# All known reranker architectures (for model type detection)
RERANKER_ARCHITECTURES = SUPPORTED_RERANKER_ARCHITECTURES | UNSUPPORTED_RERANKER_ARCHITECTURES


@dataclass
class DiscoveredModel:
    """Information about a discovered model."""

    model_id: str  # Directory name (e.g., "llama-3b")
    model_path: str  # Full path to model directory
    model_type: ModelType  # Always "llm"
    engine_type: EngineType  # "batched" or "simple"
    estimated_size: int  # Estimated memory usage in bytes


def detect_model_type(model_path: Path) -> ModelType:
    """
    Detect model type from config.json.

    Checks:
    1. architectures field for reranker-specific classes (SequenceClassification)
    2. model_type field against known embedding types
    3. architectures field for embedding-specific classes

    Args:
        model_path: Path to model directory

    Returns:
        Model type: "llm", "embedding", or "reranker"
    """
    config_path = model_path / "config.json"
    if not config_path.exists():
        return "llm"

    try:
        with open(config_path) as f:
            config = json.load(f)
    except (json.JSONDecodeError, IOError):
        return "llm"

    # Check architectures field for reranker first (more specific)
    architectures = config.get("architectures", [])
    for arch in architectures:
        if arch in RERANKER_ARCHITECTURES:
            return "reranker"

    # Check model_type field for embedding
    model_type = config.get("model_type", "")
    # Normalize: replace hyphens with underscores and lowercase
    normalized_type = model_type.lower().replace("-", "_")

    if normalized_type in EMBEDDING_MODEL_TYPES or model_type in EMBEDDING_MODEL_TYPES:
        return "embedding"

    # Check architectures field for embedding
    for arch in architectures:
        if arch in EMBEDDING_ARCHITECTURES:
            return "embedding"

    return "llm"


def estimate_model_size(model_path: Path) -> int:
    """
    Estimate model memory usage from safetensors/bin file sizes.

    MLX keeps quantized weights in compressed form, so file size ≈ memory usage.

    Args:
        model_path: Path to model directory

    Returns:
        Estimated memory usage in bytes
    """
    total_size = 0

    # Primary: safetensors files
    safetensors_files = list(model_path.glob("*.safetensors"))
    for f in safetensors_files:
        total_size += f.stat().st_size

    # Fallback: .bin files (older PyTorch format)
    if total_size == 0:
        for f in model_path.glob("*.bin"):
            # Filter out non-weight files
            name_lower = f.name.lower()
            if "optimizer" in name_lower or "training" in name_lower:
                continue
            total_size += f.stat().st_size

    # Also check in subdirectories (some models store weights in subfolders)
    if total_size == 0:
        for f in model_path.glob("**/*.safetensors"):
            total_size += f.stat().st_size

    if total_size == 0:
        raise ValueError(f"No model weights found in {model_path}")

    # Add overhead for runtime buffers (~5%)
    overhead_factor = 1.05

    return int(total_size * overhead_factor)


def discover_models(model_dir: Path) -> dict[str, DiscoveredModel]:
    """
    Scan model directory and create entries for each valid subdirectory.

    Directory structure:
    model_dir/
    ├── llama-3b/          → model_id: "llama-3b"
    │   ├── config.json
    │   └── *.safetensors
    └── qwen-7b/           → model_id: "qwen-7b"

    Args:
        model_dir: Path to directory containing model subdirectories

    Returns:
        Dictionary mapping model_id to DiscoveredModel
    """
    if not model_dir.exists():
        raise ValueError(f"Model directory does not exist: {model_dir}")

    if not model_dir.is_dir():
        raise ValueError(f"Model directory is not a directory: {model_dir}")

    models: dict[str, DiscoveredModel] = {}

    for subdir in sorted(model_dir.iterdir()):
        if not subdir.is_dir():
            continue

        # Skip hidden directories
        if subdir.name.startswith("."):
            continue

        # Check if it's a valid model directory
        config_path = subdir / "config.json"
        if not config_path.exists():
            logger.warning(f"Skipping {subdir.name}: no config.json found")
            continue

        model_id = subdir.name

        try:
            model_type = detect_model_type(subdir)
            # Select engine type based on model type
            if model_type == "embedding":
                engine_type: EngineType = "embedding"
            elif model_type == "reranker":
                engine_type = "reranker"
            else:
                engine_type = "batched"
            estimated_size = estimate_model_size(subdir)

            models[model_id] = DiscoveredModel(
                model_id=model_id,
                model_path=str(subdir),
                model_type=model_type,
                engine_type=engine_type,
                estimated_size=estimated_size,
            )

            size_gb = estimated_size / (1024**3)
            logger.info(
                f"Discovered model: {model_id} "
                f"(type: {model_type}, engine: {engine_type}, size: {size_gb:.2f}GB)"
            )

        except Exception as e:
            logger.error(f"Failed to discover model {model_id}: {e}")
            continue

    return models


def format_size(size_bytes: int) -> str:
    """Format byte size as human-readable string."""
    for unit in ["B", "KB", "MB", "GB", "TB"]:
        if abs(size_bytes) < 1024.0:
            return f"{size_bytes:.2f}{unit}"
        size_bytes /= 1024.0
    return f"{size_bytes:.2f}PB"
